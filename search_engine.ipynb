{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d16d4ab4-9fba-44e1-a596-9555f2881c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "get_ipython().run_line_magic('config', \"InlineBackend.figure_format = 'svg'\")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import re\n",
    "seaborn.set_theme()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "378464d1-7647-4f58-92a2-f62441b26c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, lowercase=True, keep_apostrophes=True, split_hyphens=False):\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "\n",
    "    if keep_apostrophes:\n",
    "        text = re.sub(r\"'\\w\\b\", \"\", text)\n",
    "\n",
    "    tokens = []\n",
    "    if split_hyphens:\n",
    "        tokens = re.findall(r\"\\w+(?:'\\w+)?|[^\\w\\s]\", text)\n",
    "    else:\n",
    "        tokens = re.findall(r\"\\w+(?:[-']\\w+)*|[^\\w\\s]\", text)\n",
    "\n",
    "    tokens = [t for t in tokens if t and t != \"'\"]\n",
    "\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a796c5a2-c313-4ac8-b174-59a59f006865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_id_to_word(tokens: list[str]) -> dict[int, str]:\n",
    "    return {i: word for i, word in enumerate(sorted(set(tokens)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7862152-bad1-4435-81a9-219dabb83fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(id, vocab_size):\n",
    "    res = [0] * vocab_size\n",
    "    res[id] = 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c76c04e-f56a-4c6d-86e2-9620e576f1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_to_id(tokens: list[str]) -> dict[str, int]:\n",
    "    return {word: i for i, word in enumerate(sorted(set(tokens)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7909dfe-6826-4a0d-a97d-ac41a98aa242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "dataset = api.load(\"text8\")\n",
    "\n",
    "tokens = [word for sentence in dataset for word in sentence]\n",
    "\n",
    "\n",
    "\n",
    "word_to_id = create_word_to_id(tokens)\n",
    "id_to_word = create_id_to_word(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a80cb5ed-bb9a-4c73-b9cf-f6b287b8bc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_readable_vectors(centre_vec: list[int], context_vec: list[int],\n",
    "                         id_to_word_map: dict[int, str]) -> str:\n",
    "    centre_word = id_to_word_map[np.argmax(centre_vec)]\n",
    "    context_word = id_to_word_map[np.argmax(context_vec)]\n",
    "    return f\"{centre_word} â†’ {context_word}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb0abbfd-e975-4631-9c3b-89bfa47cf698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(tokens, word_to_id, window=2):\n",
    "    X = [] # stores one_hot_encoded centre words\n",
    "    y = [] # stores one hot encoded context words\n",
    "    n_tokens = len(tokens) # the number of tokens\n",
    "\n",
    "    id_to_word_map = create_id_to_word(tokens)\n",
    "\n",
    "    for i in range(n_tokens):\n",
    "        left = list(range(max(0, i - window), i)) # A range of numbers to the specified distance to the left\n",
    "        right = list(range(i+1, min(n_tokens, i + window + 1))) # A range of numbers to the specified distance to the right\n",
    "\n",
    "        for j in left + right: # Loops through the tokens to the right or left (context tokens)\n",
    "            if i == j:\n",
    "                continue\n",
    "            X.append(one_hot_encode(word_to_id[tokens[i]], len(word_to_id)))\n",
    "            y.append(one_hot_encode(word_to_id[tokens[j]], len(word_to_id)))\n",
    "            #print(human_readable_vectors(X[-1], y[-1], id_to_word_map))\n",
    "\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4823f3b7-b5ed-48e2-8a4b-98c4556ad309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_network(vocab_size, n_embedding):\n",
    "    model = {\n",
    "        \"w1\": np.random.randn(vocab_size, n_embedding),\n",
    "        \"w2\": np.random.randn(n_embedding, vocab_size)\n",
    "    }\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ac5a49-bc52-4170-a804-cec2a20128ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data\n",
    "X, y = generate_training_data(tokens, word_to_id, window=2)\n",
    "\n",
    "# Get vocabulary size\n",
    "vocab_size = len(word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cf8ae5-ed12-4456-907b-aaf2c1c1e801",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, epsilon=1e-10):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # Numerical stability\n",
    "    return exp_x / (np.sum(exp_x, axis=1, keepdims=True) + epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f864b85c-9e83-429d-a395-9c3e956f65c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(z, y):\n",
    "    z = np.clip(z, 1e-10, 1-1e-10)  # Avoid log(0)\n",
    "    return -np.sum(y * np.log(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4d5412-4de2-44ed-86ca-c0e69394ef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(model, X, y, alpha, clip_value=5.0):\n",
    "    cache = forward(model, X)\n",
    "\n",
    "    # Gradient calculations\n",
    "    da2 = cache[\"z\"] - y\n",
    "    dw2 = cache[\"a1\"].T @ da2\n",
    "    da1 = da2 @ model[\"w2\"].T\n",
    "    dw1 = X.T @ da1\n",
    "\n",
    "    # Gradient clipping\n",
    "    dw1 = np.clip(dw1, -clip_value, clip_value)\n",
    "    dw2 = np.clip(dw2, -clip_value, clip_value)\n",
    "\n",
    "    # Update weights\n",
    "    model[\"w1\"] -= alpha * dw1\n",
    "    model[\"w2\"] -= alpha * dw2\n",
    "\n",
    "    return cross_entropy(cache[\"z\"], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460797c4-1915-4c63-aec9-a97cfbb186d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(model, X, return_cache=True):\n",
    "    cache = {}\n",
    "    cache[\"a1\"] = X @ model[\"w1\"]\n",
    "    cache[\"a2\"] = cache[\"a1\"] @ model[\"w2\"]\n",
    "    cache[\"z\"] = softmax(cache[\"a2\"])\n",
    "    return cache if return_cache else cache[\"z\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec30a22-d182-4144-94cf-d76bc3d63bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embedding = 10\n",
    "model = init_network(vocab_size, n_embedding)\n",
    "\n",
    "# Training parameters\n",
    "n_iter = 50\n",
    "learning_rate = 0.05\n",
    "\n",
    "history = []\n",
    "for i in range(n_iter):\n",
    "    loss = backward(model, X, y, learning_rate)\n",
    "    history.append(loss)\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Iteration {i}: Loss = {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a5c0f9-be78-4143-ab1e-632d6ec1cd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(len(history)), history, color=\"skyblue\")\n",
    "plt.title(\"Training Loss Over Iterations\", pad=20)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bd0c53-68f5-4d8b-a444-8719282e61e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning = one_hot_encode(word_to_id[\"deepmind\"], len(word_to_id))\n",
    "result = forward(model, [learning], return_cache=False)[0]\n",
    "\n",
    "for word in (id_to_word[id] for id in np.argsort(result)[::-1]):\n",
    "    print(word)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
